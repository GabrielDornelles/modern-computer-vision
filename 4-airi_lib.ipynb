{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airi\n",
    "Autor: Gabriel Dornelles Monteiro, abril de 2022. Notebook nº3.5.\n",
    "\n",
    "Construimos dois modelos nos notebooks anteriores, e como vimos, o processo para construir e treinar os estes modelos cresceu muito do primeiro para o segundo, se torna bastante impráticavel aumentarmos os modelos com o paradigma atual que utilizamos até agora. \n",
    "\n",
    "Para isso, iremos fazer o mesmo que os modernos frameworks de Machine Learning fazem, iremos modularizar todo nosso processo, isto é, criaremos módulos que executam o que precisamos, calculam o que deve ser calculado, para que dessa maneira, nosso trabalho seja encaixar estes módulos como encaixamos peças de Lego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira geral, cada um de nossos blocos terá o seguinte formato:\n",
    "\n",
    "```py\n",
    "class AiriLayer:\n",
    "    def __init__(self):\n",
    "       pass\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self, **kwargs):\n",
    "        pass\n",
    "```\n",
    "Onde :\n",
    "- em  ```__init__``` temos um construtor padrão para nossa classe\n",
    "- Utilizamos o magic method ```__call__``` para passar o método forward, ou seja, ao executarmos algo como:\n",
    "```py\n",
    "sample_module = AiriLayer()\n",
    "sample_module(x)\n",
    "```\n",
    "Estaremos efetivamente, executando o método forward da classe.\n",
    "- Em ```forward``` escreveremos o código que performa a computação de inferência do modelo\n",
    "- Em ```backward``` escreveremos o código que calcula os gradientes para nosso bloco, e também que ira retornar o gradiente que deve seguir seu fluxo pelo modelo\n",
    "- Em ```update``` escreveremos o código que irá realizar efetivamente todos os updates dos parâmetros pertencentes a nosso bloco.\n",
    "- Em ```zero_grad``` iremos limpar a memória zerando todos os parâmetros que mantivemos em cache durante o processo de otimização.\n",
    "\n",
    "----\n",
    "\n",
    "Vejamos o exemplo de implementação para nosso bloco Linear, que é nossa matriz de pesos:\n",
    "\n",
    "```py\n",
    "class Linear(AiriLayer):\n",
    "\n",
    "    def __init__(self, input_size=3072, hidden_size=10, reg=1e3, bias = True):\n",
    "        self.config = None\n",
    "        self.config_b = None\n",
    "        self.bias = bias\n",
    "        std = 1./ math.sqrt(input_size)\n",
    "        self.w =  np.random.randn(input_size, hidden_size) * std\n",
    "        self.b = np.zeros(hidden_size) if bias else None\n",
    "        self.reg = reg\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x, grad = True):\n",
    "        if grad: \n",
    "            self.x = x\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        self.dW = self.x.T@dout\n",
    "        self.dB = dout.sum(axis=0) if self.bias else None\n",
    "        self.dW += self.reg * 2 * self.w \n",
    "        return dout@self.w.T \n",
    "        \n",
    "    def update(self, lr=1e-3):\n",
    "        self.w -= lr * self.dW\n",
    "        self.b -= lr * self.dB\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "        self.x = None\n",
    "```\n",
    "\n",
    "Veja que no construtor, inicializamos a matriz normalmente, como iremos treinar modelos maiores, utilizaremos a Kaiming Initialization. Perceba que simplesmente modularizamos o processo que construimos no Softmax Classifier, de maneira a deixar isto incrementável/escalável.\n",
    "\n",
    "Para nossa layer Softmax:\n",
    "\n",
    "```py\n",
    "class Softmax(AiriLayer):\n",
    "\n",
    "    def __init__(self, loss: str = \"NLL\"):\n",
    "        self.loss_function = loss\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, scores, grad = True):\n",
    "        self.batch_size = scores.shape[0]\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        scores_exp = np.exp(scores)\n",
    "        softmax_matrix = scores_exp / np.sum(scores_exp, axis=1, keepdims=True) \n",
    "        if grad:\n",
    "            self.softmax_matrix = softmax_matrix\n",
    "        return softmax_matrix\n",
    "    \n",
    "    def NLLloss(self, y):\n",
    "        loss = np.sum(-np.log(self.softmax_matrix[np.arange(self.batch_size), y]))\n",
    "        loss /= self.batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y):\n",
    "        if self.loss_function == \"NLL\":\n",
    "            self.softmax_matrix[np.arange(self.batch_size) ,y] -= 1\n",
    "            self.softmax_matrix /= self.batch_size\n",
    "            return self.softmax_matrix\n",
    "        raise NotImplementedError(\"Unsupported Loss Function\")\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.softmax_matrix = None\n",
    "```\n",
    "\n",
    "Novamente, não há nada de novo, o código aqui presente é copiado e colado do código que trabalhamos anteriormente.\n",
    "\n",
    "\n",
    "----\n",
    "O treinamento do Softmax Classifier agora é reduzido a:\n",
    "```py\n",
    "from layers import Linear, softmax\n",
    "\n",
    "model = [\n",
    "    Linear(input_size=3072, hidden_size=10, reg=1e3, bias=True),\n",
    "    Softmax()\n",
    "    ]\n",
    "\n",
    "n_epoches = 500\n",
    "x = train_images\n",
    "y = train_targets\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    \n",
    "    for layer in model:\n",
    "        x = layer.forward(x) # Não é necessario chamar o método forward. layer() é suficiente\n",
    "    \n",
    "    loss = model[-1].NLLloss(y)\n",
    "    dout = model[-1].backward(y)\n",
    "    \n",
    "    for layer in reversed(model[:-1]):\n",
    "        dout = layer.backward(dout)\n",
    "        layer.update()\n",
    "        layer.zero_grad()\n",
    "```\n",
    "\n",
    "Da mesma maneira, se quisermos agora treinar nossa Two Layer Neural Net, basta cria-la com blocos:\n",
    "\n",
    "```py\n",
    "model = [\n",
    "    Linear(input_size=3072, hidden_size=100, reg=1e3, bias=True),\n",
    "    Relu()\n",
    "    Linear(input_size=100, hidden_size=10, reg=1e3, bias=True),\n",
    "    Softmax()\n",
    "]\n",
    "```\n",
    "\n",
    "E basta treinar da mesma maneira.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convoluções\n",
    "\n",
    "O processo de convolução já foi amplamente utilizado em processamento de imagem, como filtros de detecção de borda (Sobel, Canny), filtros para borrar a imagem, dentre muitos outros. De maneira simples, o processo de convolução é dado por analisar uma pequena região da imagem repetidas vezes:\n",
    "\n",
    "![full_padding_no_strides_transposed](https://user-images.githubusercontent.com/56324869/118333250-c1fa9c00-b4e1-11eb-99ac-d221e285976a.gif)\n",
    "\n",
    "Onde criamos uma matriz quadrada (normalmente, mas retangulares também são usadas) e essa possui valores arbitŕarios, de maneira que conseguimos certos resultados com diferentes valores nesta matriz.\n",
    "\n",
    "O exemplo base é o filtro sobel, dado pelas matrizes:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/56324869/118333675-7e546200-b4e2-11eb-9cbc-32a0300dbb98.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/56324869/118334477-1272f900-b4e4-11eb-9dfb-2d7d2e2e2173.png)\n",
    "\n",
    "Que calcula as distâncias entre as cores, de maneira vertical e horizontal (cada matriz faz uma direção), dessa maneira, temos como resultado 0 quando não há diferença de cor na regiao analisada, e 1 (ou nesse caso 255) onde há uma diferença extrema de cor. Por isso, temos o filtro sobel como um detector de bordas.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/56324869/163439169-d92e254c-7e38-4d55-b7b0-09081396017a.png)\n",
    "\n",
    "Em nossos modelos, vamos utilizar os valores dentro dessa matriz como parâmetros que serão aprendidos pelo modelo, sendo então sua função aprender filtros que sejam relevantes para mostrar as caracterísisticas das imagens. Para isso, em cada camada de convolução, temos N número de filtros MxM, como por exemplo, 32 filtros 3x3, e o resultado dessas imagens é que iremos passar para nossas camadas lineares. \n",
    "\n",
    "Você pode ver uma implementação do código para o forward de uma camada de convolução nas layers de nossa lib, e também o seu backward. Como esta é uma operação mais custosa, a implementação utiliza Cython e necessita de um build. A mesma foi retirada do curso cs231n de Stanford, e re-implementada de maneira modular dentro de nossa lib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando nossa ConvNet\n",
    "\n",
    "O modelo que iremos rodar se encontra no script train.py dentro da lib airi. \n",
    "\n",
    "Modelo:\n",
    "\n",
    "```py\n",
    "    self.model = [\n",
    "            Conv2D(in_channels=3, num_filters=16,filter_size=5, stride=1, pad=0),\n",
    "            Relu(),\n",
    "            Conv2D(in_channels=16, num_filters=16, filter_size=5, stride=1, pad=0),\n",
    "            Relu(),\n",
    "            Flatten(),\n",
    "            Linear(input_size=9216, hidden_size=120, reg=self.reg),\n",
    "            Relu(),\n",
    "            Linear(input_size=120, hidden_size=84, reg=self.reg),\n",
    "            Relu(),\n",
    "            Linear(input_size=84, hidden_size=10, reg=self.reg),\n",
    "            Softmax()\n",
    "        ]\n",
    "```\n",
    "\n",
    "Iremos treinar o modelo por 20 épocas, com um batch size de 128, porém não mais e maneira aleatória. Dessa vez faremos o processo correto, que é:\n",
    "\n",
    "- Iteramos o dataset inteiro em batches de 128, cada vez que passamos 128 exemplos, calculamos os gradientes e aplicamos o update. \n",
    "- Fazemos isso para todo o dataset 20 vezes.\n",
    "\n",
    "Fazendo isso, verá que nosso algoritmo para otimização é extremamente fraco. Dado que implementamos tudo em python, nossa execução é muito lenta, e levamos cerca de 3 minutos e meio por época para treinar nosso modelo (na minha máquina). O nosso SGD é muito simples e pouco elaborado, por isso vamos ver um meio de otimizar nosso modelo de maneira mais eficiente\n",
    "\n",
    "---\n",
    "\n",
    "# Otimizadores\n",
    "\n",
    "O SGD é bom, mas é muito lento, adicionaremos momentum, o que dará aceleração para ficar mais rápido quando encontrar uma boa direção em relação ao minimo da loss function.\n",
    "\n",
    "\n",
    "## SGD + Momentum\n",
    "\n",
    "Em particular, a perda pode ser interpretada como a altura de um terreno montanhoso (e, portanto, também para a energia potencial, pois U=mgh e, portanto, U∝h ). Inicializar os parâmetros com números aleatórios é equivalente a definir uma partícula com velocidade inicial zero em algum local deste terreno. O processo de otimização pode então ser visto como equivalente ao processo de simulação do vetor de parâmetros (ou seja, uma partícula) rolando na montanha.\n",
    "\n",
    "Adicionar momentum ao SGD significa que estamos adicionando aceleração ao gradiente (análogo à física, onde f = ma, então o gradiente (negativo) é nesta visão, proporcional à aceleração da partícula). Temos uma interpretação física como uma bola rolando pela montanhosa função de perda, dessa vez adicionamos o atrito (coeficiente mu) e também a aceleração.\n",
    "\n",
    "```py\n",
    "## vanilla SGD ##\n",
    "x += learning_rate * dx\n",
    "########################\n",
    "\n",
    "## SGD + Momentum ##\n",
    "v = 0 # initialize velocity at 0\n",
    "# momentum update\n",
    "# mu usually something between 0.5 and 0.99\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position\n",
    "```\n",
    "\n",
    "Normalmente, o momentum é aumentado em estágios posteriores de aprendizagem. Uma configuração típica é começar com momentum de cerca de 0,5 e gradualmente aumenta-lo para 0,99 ao longo das épocas.\n",
    "\n",
    "## AdaGrad\n",
    "\n",
    "Per parameter **Ada**ptive learning rate method for the **grad**ients. Este otimizador tem como objetivo trazer algo como um learning rate próprio para cada parâmetro do modelo, que é aumentado ou diminuido dinamicamente baseado nos últimos gradientes calculados pelo modelo. Para isso, ele adiciona o quadrado dos parâmetros em uma especie de cache:\n",
    "\n",
    "```py\n",
    "# AdaGrad update\n",
    "cache += dx**2\n",
    "x +=  - learning_rate * dx  / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "\n",
    "Ao treinar por muito tempo, o update que fazemos se aproxima muito de zero, já que o cache é incrementado toda vez, eventualmente ele é grande demais, fazendo a operação do nosso update ser dividida por um valor muito alto, resultando em pouco update, e eventualmente nenhum update.\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "Um otimizador baseado no treinamento em batches. Seu objetivo é corrigir o problema do AdaGrad, para isso, adiciona uma variavel **decay_rate**, que irá vazar um pouco do cache durante o processo de otimização, dessa maneira, o tempo para que se chegue a updates nulos é mais lento comparado ao AdaGrad.\n",
    "\n",
    "```py\n",
    "# RMSProp\n",
    "cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x +=  - learning_rate * dx  / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "\n",
    "## Adam\n",
    "**Ada**ptative **M**oment estimation\n",
    "\n",
    "O querido de todos, e amplamente utilizado na esmagadora maioria de modelos. Este otimizador é uma versão do RMSProp com a utilização de Momentum, e iremos utiliza-lo para treinar nossas ConvNets.\n",
    "\n",
    "Sua ideia é algo como:\n",
    "\n",
    "```py\n",
    "# update first moment (momentum like), beta1 is like the friction we had in Momentum SGD.\n",
    "m = beta1*m + (1-beta1)*dx \n",
    "\n",
    "# update second moment (RMSProp like), where we had a cache: \n",
    "v = beta2*v + (1-beta2)*(dx**2) \n",
    "\n",
    "# Adam update\n",
    "x += - learning_rate * m / (np.sqrt(v) + 1e-7)\n",
    "```\n",
    "\n",
    "\n",
    "# Veja a diferença \n",
    "\n",
    "Veja como os diferentes otimizadores performam a descida de gradiente:\n",
    "\n",
    "![3_-_NKsFHJb_-_Saddle_Point](https://user-images.githubusercontent.com/56324869/163443678-c1b4a5dc-3c02-4ca4-afce-856cccf99001.gif)\n",
    "\n",
    "![1_STiRp7PW5yIrvYZupZA6nw](https://user-images.githubusercontent.com/56324869/163443917-d30dc066-1dfb-4c25-97c1-ecff45d01de5.gif)\n",
    "\n",
    "Este é um campo com estudos ativos sempre, e verá que há outros como AdamW, dentre outros propostos:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/56324869/163444019-803cfcca-5f21-442a-8aa0-e01cd83b6d1b.gif)\n",
    "\n",
    "De maneira geral, Adam e RMSProp são amplamente utilizados hoje em dia tanto na academia quanto nos projetos reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento\n",
    "\n",
    "Para executar o treinamento da nossa ConvNet, basta executar o script train.py na lib airi. Iremos utilizar o módulo **pickle** do python para serializar nosso modelo em um arquivo binário e carrega-lo facilmente quando quiseremos (isto é exatamente o PyTorch faz com seus modelos!). \n",
    "\n",
    "Veremos os seus resultados no próximo notebook!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
